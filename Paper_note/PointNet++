Vac. 
partition：划分  metric space：度量空间  recursively：迭代地 nested:嵌套的 intriguing：有趣的  entanglement：纠葛 counterpart:对方
counter:相反的   deﬁciency：缺乏  leverages ：use something使得最大化利益 coverage：覆盖范围  ﬁxed length：固定长度
vicinity：附近  concatenated：级联的 distributional：分布的 interpolation：插值
(一)摘要
PN没有获取到点存在的度量空间中引入的局部结构信息，限制了他识别具有好的颗粒度模型和泛化到复杂场景的能力。PN++通过利用度量空间距离。能够
随着增加的contextual scales学习到局部特征。点集通常都是由不同的密度采样获得，这导致了在统一密度下训练的网络会有很差的表现。提出新颖的
learning layers可以适应性地结合来自不同scales的特征。实验表明PN++可以学习到点集深层的特征。PN++引入了一个多层次神经网络，这个网络
把PN迭代地应用在点集的嵌套分割上。

（二）介绍
1）分析欧几里得空间里点的集合的几何结构是有必要的，距离度量下定义的局部邻域可能会产生不同的性质。比如在不同的位置下点的密度和属性可能是不
统一的。
2）PN的基本思想是学习每个点的空间编码然后将每个点的特征集合成一个全局点云signature。这个方法PN没有捕捉到局部的几何信息。CNN将规则网络下
定义的数据作为输入，然后能够逐步地通过一个多分辨率的层次，对着scales的增加捕捉到特征。低层次的神经元有较小的感受域，而高层次的有更大的
感受域。
3）PN++用来处理在度量空间下采样到的点集的层次化方法。首先将根据底层的空间度量将点集分为不同的重叠局部区域。和CNN很像，我们从小的邻域提出到好的
集合结构信息。这样的局部信息分类进更大的单元然后处理成更加高层次的特征信息。
4）PN++提出了两个问题，如何产生点集的分类和如何通过局部的feature learner提取到点集的局部特征。这两个问题是相关的，点集的分类必须具有跨分类的
统一结构，这样feature learner才能分享权重。我们选择feature learner为PN。PN是一个处理无序无序点集然后提取语义特征的有效结构。PN++迭代地把
PN应用在输入集合的嵌套分类。
5）如何产生点集的重叠分类。每个分类是在底层欧几里德空间定义成的一个邻域球，这个邻域球包括参数中心点的位置和大小。为了平均地覆盖到
整个集合，中心点是由farthest point sampling算法得到的。比起体积CNN通过固定的步幅来扫描空间的方法，这样子的局部感受域能够依赖于输入的数据和
度量空间。（主要是这个算法可以根据输入的数据分布进行点集的分类。）
6）决定局部邻域的大小：我们认为输入点集可能在不同区域有不同的密度。我们的输入点集和CNN输入非常不同，CNN输入可以看作是在规则网络上使用统一
的不变密度定义的（？）。在CNN中， 与局部分类大小相反的一方是核大小。使用小的核能够更好地提高CNN的表现。然而论文的实验表明小的邻域可能因为
涵盖太少的点导致采用不完全，使得PN不能鲁棒性地获取到特征。网络学会自适应地对在不同尺度下检测到的模式进行加权，并根据输入数据组合多尺度特征
。
（三）网路结构
1）我们的工作可以看作是PN和额外的层次化结构。
2）PN只有提取无序点集的特征的能力，如果直接将整个点集作为PN的输入的话只能得到整个点集的某个特征的信息。PN++通过将输入进行划分，通过分享
同样的PN，对点集A，点集B同时进行特征提取。这样分别提取到了两个点集的特征A，特征B，然而这两个特征是不带有整体的几何信息的。将这两个特征
重新集合在一起变成点集C的时候，作为输入再次提取特征的时候就提出了一定范围内的几何信息（什么时候引入的几何信息呢？是因为没有一次性进行
最大化操作，而是结构化地一次次进行最大化操作进行特征提取的原因吗 将点A，点B分为一类进行PN操作的时候，最后得到的会是含有点A点B的特征信息的向量，
之后再将这个向量进行处理就会融合AB信息。这个过程有获得几何信息吗？提取到几何信息的原因是grouping的过程中是按照几何方法来group的吧？）
3）Sampling层：从输入的点选择子点集，决定了局部区域（local region）的中心点。与扫描与数据分布无关的向量空间的CNN相比，
我们的采样策略以依赖数据的方式生成接受域。（以依赖数据的方式生成接受域很关键）
4）Grouping层：通过寻找中心点附近相邻的点来创建局部区域集合。
5）Pointnet层：使用mini-Pointnet来解码局部区域图示，输出特征向量和中心点的坐标信息。（表示之后处理的特征都会和中心点的信息连接在一起，
也就是局部特征和在空间的几何位置结构就想挂钩的）输入处理的每一个点对会减去中心点的坐标信息，获得归一化结果。
6）在稀疏的区域就需要在更大的区域找到更大规模的样式。为了实现这个目标，我们提出了密度自适应的PN层，来学习如何结合来自不同大小区域的特征。
7）所有点的坐标---(使用FPS选出子集sampling）--->N个点的坐标和N`个中心点的坐标----（选出每个中心点的邻域，每个邻域包含K个点grouping）
-->N`*K个点---（pointnet）-->输出为N`*(C`+d)
8）Multi—scale Grouping：一个简单有效的获得多尺寸样式的方法是将grouping layer应用在不同的尺寸上，然后由下来的PN提炼每个尺寸的特征。
来自不同尺寸的特征连接形成一个多尺寸特征向量。MSG的计算消耗很大，在大尺寸邻域中也会跑PN，同时中心点很多的时候，计算量就很大。
9）Multi-resolution Grouping多分辨率组合：这个方法并没有使用多尺寸来进行grouping，而是通过特征提出的结果提出到的是不同分辨率下，
然后通过控制权重来解决点稀疏和点密集的情况。点稀疏的时候我们就不求高分辨率的做法，直接将所有的点都进行特征提取。如果点密集的话我们就可以
高分辨率来提取特征。
每层对某个局部区域的特征提取由两部分组成: 基于上一层输出的特征提取到的特征+该区域对应的原始点集提取出的特征. 前者经过了两层特征提取,感受野更大, 
适用于比较稀疏的点集. 而后者只做了一次适用于比较稠密的点集.

（四）下采样导致的问题
总结：为了提高鲁棒性，文章从PN没有收集到的结构信息入手，通过迭代的网络不断扩展局部的特征提取，提高感受域的大小后获得更好的结构特征。

（五）解决姿态问题（感觉这是因为放弃了PN中特征对齐等做法的问题）
改变输入数据的度量空间，即是改变输入数据的形式，使得输入的数据对姿态变换问题是不敏感的。
