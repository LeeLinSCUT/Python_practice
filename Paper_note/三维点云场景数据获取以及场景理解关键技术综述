一、引言
（1）场景理解主要包含目标检测与识别、物体间的关系以及语义分割等。
（2）2D图像数据（RGB，遥感图像）提供了物体的颜色、纹理以及光谱等关键特征。但是受各种条件影响。
     3D建模数据(点云、CAD模型）能够感知场景的3D空间信息。
二、点云数据获取
(3)获取3D重建数据的传感器：普通摄像头、全景摄像机系统、深度相机和激光雷达等。3D点云的获取设备（Kinect、TOF、激光雷达）
（4）3D数据重建方法：i)基于立体视觉 ii)激光  iii)视觉与激光联合重建 
基于立体视觉的3D重建方法模仿人眼的"视差原理"（多目视觉方案）当外界光线由强变弱，物体表面不清晰的时候容易产生误匹配。（主要是受环境的影响）。Kinetc和TOF摄像头拍摄范围较小。
激光3D重建的测距较远，但是只有场景的几何结构信息，缺乏场景的彩色信息。
基于合成孔径雷达成像和高清影像可以生成地面场景的点云数据。具有颜色信息而且为大尺度场景。但是地面底部信息精度不高，会有一定的遮挡和信息缺失。
利用相机和激光点云的联合3D重建可以得到大尺度场景的几何结构信息和场景的彩色信息。
基于激光扫描与全景影像的车载测量集成系统，采用了高精度全球定位技术。无法应用与室内。
固定式激光扫描可以用于室内室外的环境，复杂场景需要大量换站进行点云拼接。
即时定位与地图构建（SLAM）在移动的3D重建问题上具有较好的应用，使用了2D的SLAM技术。

（Kinect传感器主要由深度摄像机和RGB摄像机来获取外界的三维信息。深度摄像机由红外投影机和红外深度摄像机构成。红外投影机主要用于向外界发射均匀的红外线，并在目标上形成红外散斑图像；
目标反射得到的散斑图像信息由红外深度摄像机接收，最后得到目标的深度图像。RGB摄像机主要获取实验环境或实验物体的RGB图像信息。）
三、3D场景数据类型对比及数据库
（5）不同的数据类型进行场景理解时具有不同的效果。
2D RGB图像                    2D信息，彩色结构                                    图像对光源敏感，缺少深度信息，不能直接获取几何信息

基于图像3D重建的点云数据        3D场景、彩色结构和其他有深度信息、                   点云对光和婚嫁敏感，重建信息是缺失的
                              距离的稀疏点云

基于Kinect的深度点云数据        3D室内小场景，密集点云                              点云数据对光源敏感，只能对小场景。
         
车载激光雷达的点云数据          高精度，密集点云。3D空间信息和亮度信息。受环境影响小。  受平台限制，只有线性公路轨迹场景的点云场景才可以
                                                                                 无法提取场景颜色信息。
                                                      
静止激光雷达的点云数据          3D室外场景，高精度，密集点云，3D空间信息和亮度。       点云信息不完整。

航拍激光雷达的点云数据

基于碰撞的全景图像和            3D外部场景，高精度信息，密集点云。移动建模，全景3D    场景颜色信息不能单独由激光雷达点云给出。
激光点云融合数据                空间，颜色和亮度信息。

定义具有颜色信息的激光点云为彩色激光点云，该数据通过RGB图像和激光点云联合3D建模生成。

三、3D场景数据库（
基于高程的滤波方法：根据点分布在点云中进行滤波，手动或自适应地找到z方向的阈值滤除z值小于某个值的点 （快，但是低鲁棒性）
基于模型的滤波方法：使用一个模型去fit ground(适合特定的场景，效果很好但是低鲁棒性）
基于区域生长的方法：将点的法向量作为区域生长准则，首先自适应地找到最可能是地面的点。基于此，基于临近的点的法向量和这个点的角度不同
判断是否区域生长。（如果地面不是起伏的，那有很好的效果但是速度相对较慢）
基于滑窗的方法：分布在地面上的点应该是具有连续特性的，设置一个设置的窗口大小找到现在窗口中最低的点，由最低点计算模型设置的阈值来滤除
所有高度差超过阈值的点。 （更快，但是窗口大小依赖手动设置，而且只考虑到了local feature）
基于三角网的滤波方法：离散的点是根据多个覆盖整个区域的三角形相连接的（？），这些三角形相互不重叠形成一个不规则的三角网。稀疏uTIN是由
seed point生成的。模型的斜率用来实现最初的分割然后去除掉斜率。大的三角区域，通过连接性分析获取每一个部分之间的特征比如高度差。
（避免了当地形平坦的时候数据冗余，但是数据结构很复杂而且空间复杂度高。

ii）点云的特征提取与分割
点云稳健特征的提取能够较好的表征场景内的目标对象，有效点云块的初始化分割减少了后续场景理解工作的计算量。
局部特征：i）关键点p的单元法线向量n作为局部的参考轴来获得旋转图像说明器 ii)扩展了2D形状上下文特征描述到3D点云中获得3D的形状上下文特征描述。
iii)提出一个点特征直方图表征算子（PFH） iv）（SHOT）提出signature histograms of orientation feature，对噪声鲁棒。
全局特征：i)Ensemble fo Shape Functions（ESF）计算一个目标点云的三个特征：角度，点距离和区域 ii）Viewpoint Feature Histogram
继承于FPFH描述子，FPFH扩展到将计算用于整个点云结构，然后添加额外的每个点视点方向和估计法线方向之间的数据。
基于边缘分割：提出边缘检测策略，从2D边缘图中提取闭合轮廓实现快速分割。
基于区域分割：提出了基于区域生长的平面分割算法
基于模型分割：fitting a plane with ramdom sampling consistency algorithm(?)  基于局部采样和统计推断的点云分割算法。
基于图分割：结合空间、几何和外观特征作为图边缘的计算方法。/使用图模型来perform区域融合，获得清晰边界的RGB-D 分类图。
基于簇分割 航拍图像的光谱信息分配给空中点云，光谱信息作为特征向量分簇，来分离建筑物提取轮廓。/介绍了使用超像素和机器学习的语义分割，
使用clustering 和 conditional random field algorithm来实现点云数据分类和场景理解。
  

iii）3D点云的语义理解
语义分割是场景理解的基础和前提，在语义分割的基础上，结合其他信息继续推理得到其他高层语义。如场景类别，场景发生的事情等等
点云分类算法主要分类两大类，基于点的分类：直接对每一个3D点进行分类。计算每个点的特征向量使用分类器训练来对室外场景点进行直接分类（一个算法）
基于分割的分类：首先对点云数据进行分割，再判别每个分割快的类别。如根据法线信息对每个点构建邻接关系，组成大的分割块，再利用SVM判别来
实现城市道路场景的点云分类。这类基于机器学习的方法有效的特征描述是关键内容。
基于深度学习的点云理解主要面临三个挑战： 1）点云数据是分布在空间中任意点的非结构化数据。没有结构化的网络而无法直接使用卷积神经网络。
2）相同的点云可以由完全不同的矩阵表示 3）点云数量不确定
深度学习网络分为三大类
1)基于2D投影的深度学习网络：将3D点云投影到2D图像后作为CNN的输入。常用的2D投影图像有基于虚拟相机的RGB图像，基于虚拟相机的深度图，基于传感器
获取的距离图像和全景图像等映射图像。 VGG-16,AlexNet，Googlenet，FCN,U-NET。MVCNN、Snapnet等方法采用多视角投影的方法。容易造成
3D结构信息的丢失。
2）基于3D体素化的CNN：对点云进行体素化等预处理操作，提出基于3D CNN模型。OctNet通过非平衡八叉树和3DCNN对点云进行了高分辨率体素表征，
表征后的结果在目标分类、点云标记等工作中进行了验证。体素化的方法：i)有基于0-1表示是否有点的体素方法ii）基于体素网络密度的方法 iii）
基于网格点的方法 仅考虑到了点云的结构信息，没有考虑到点云的颜色、强度等信息
3）基于点云中单个点的网络模型
  为了充分利用点云的多模态信息，减少预处理过程中的计算复杂度，基于点云中单个点的网络模型逐渐被提出。
  Charles针对室内的点云场景提出了PointNet，对室内点云数据进行了分类、部分分割、语义分割3部分工作，而后针对PointNet中局部特征信息进行了
  改进，推向多尺度，综合局部特征提出了PonitNet++网络模型。Li在PointNet基础上进行改进，提出PointCNN，基于点云中点学习到的X变换方法，
  然后将其同于同时加权与点关联的输入特征。
  
 仍需重要研究的几点：
 1）针对基于学习方法需要大量有标注的样本，海量3D数据，尤其是彩色激光点云数据，人工标注效率低。需要研究快速的高精度的标注方法。
 2）3D点云场景理解需要对点云进行大量的预处理，增加算法复杂度。构建出由点到点标签的稳健性的网络模型是重要的研究方向。
 3）各类目标的点数不均衡，导致在深度学习网络模型训练时存在样本不平衡问题。解决样本训练不均衡。
 4）目前获取3D点云场景数据的精度和点云与颜色配准的效果还有待进一步提高。
 5) 对场景中各个目标的拓扑关系，场景中目标的行为与分析等内容的理解。
