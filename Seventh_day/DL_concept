(1)Logistic回归
  Logistic回归是一种广义线性回归，模型形式是w`x+b,其中w和b是待求参数。回归通过函数L将w`x+b对应一个隐状态p，p=L（w`x+b),然后根据p与1-p
  的大小决定因变量的值。如果L是logistic函数（sigmoid)，就是logistic回归。
(2)Logistic回归损失函数
  L(y^,y）=-（ylogy^+(1-y)log(1-y^))，如果当y=1的时候，我们希望y^越接近1，如果当y=0的时候，希望y^也越接近0.
（3）梯度下降法（Gradient Descent)
  w_new = w - learning_rate * gradient
（4）激活函数
  tanh_function,sigmoid_function:缺点是当输入的激活值很大或者很小的时候，值的梯度都趋于0，这样会减慢梯度下降的速度。可以作为输出函数的
  激活函数。
  relu_function:a=max(0,z) 修正线性单元。
 （5)权重随机初始化（Random Initialization)
 （6）TrainSet/DevelopmentSet（验证集）/TestSet
 （7）偏差和方差的权衡问题（Bias and Variance）
  高偏差导致欠拟合，高方差导致过拟合。偏差是预测值和实际值的关系，方差是预测值和预测值之间的关系。方差太大即使模型离散到了小众数据。
  偏差（bias）：偏差衡量了模型的预测值与实际值之间的偏离关系。通常在深度学习中，我们每一次训练迭代出来的新模型，都会拿训练数据进行预测，
  偏差就反应在预测值与实际值匹配度上，比如通常在keras运行中看到的准确度为96%，则说明是低偏差；反之，如果准确度只有70%，则说明是高偏差。
  方差（variance）：方差描述的是训练数据在不同迭代阶段的训练模型中，预测值的变化波动情况（或称之为离散情况）。
  从数学角度看，可以理解为每个预测值与预测均值差的平方和的再求平均数。通常在深度学习训练中，初始阶段模型复杂度不高，为低方差；
  随着训练量加大，模型逐步拟合训练数据，复杂度开始变高，此时方差会逐渐变高。
  （8）L2正则化Regularization（减少过拟合，减少方差）
      作用于成本函数，在后面加上正则化公式。L2正则化，使用了向量参数W的L2范数。如果使用L1正则化的话，很多w变量会变得稀疏。可能是有利于
      压缩模型。有一个超参数叫正则化参数。
      正则化的加入可以避免权重矩阵过大，使得一些权重参数接近于0，消除或者减弱了一些隐藏单元的影响。过拟合的原因就是网络学习能力太强了以至于能够过分地
      拟合训练集，正则化的加入给了简化网络的可能性，同时又不影响网络的结构。
  （9）Dropout正则化 
      原因：直接给我们的感觉是每个单元在每次迭代的过程中，都会随机连接到输入的Feature，因此不能依赖到任何一个Feature。会有收缩权重
      的效果，实施Dropout也会让权重压缩。
  （10）其他正则化方法：数据增强，Early Stopping，
  （11）正则化输入（Normalization）
    （i)减去均值 （ii)归一化方差 如果输入的值不平衡的话，会导致不同layer的权重取值的范围差别很大。可能导致优化的函数是一个狭长的碗（
    不平衡），然后导致梯度下降的速度变慢，对开始优化的起点选择也不友好。正则化后可以让整个cost function看起来更加平衡。
  （12）
  （13）
  （14）
  
  
