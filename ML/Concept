（1）机器学习算法分为：监督学习，非监督学习，强化学习，推荐系统。
（2）监督学习：包含“正确答案”的数据集，算法的目的就是得出更多的正确答案。
（3）非监督学习：聚类问题和鸡尾酒会问题（音频的分离)
（4）Regression：Predict continuous valued output，线性回归的Cost Function总是趋于Convex Fucntion（凸函数）
（5）Cost Function（代价函数):是关于假设空间的一个函数，是关于假设空间的一个评价准则。最小化代价函数除了梯度下降（迭代的方法）
  正规正方程组法。
（6）矩阵的乘法能够将多个同样的输入特征（行的形式）运用到不同的线性计算（竖的形式）中。
（7）（Linear Regression)线性回归中一个参数对应一个特征量的计算，不同的样本都是这一个位置的特征量和同一个位置的参数操作。并且受该输入的特征量影响到梯度下降。 
（8）感觉特征缩放类似于Normalization。
（9）多项式回归：假设函数用的是多项式，输入的特征不是经过线性的处理了而是非线性的。 同一个特征经过平方后重新提取出新的数据信息配合新的
参数计算提取特征。
（10）Normal equatin（正规方程）：Method to solve for θ analytically
（11）Logistic Regression：是一个分类算法，线性回归会产生远大于1或者远小于0的值不利于分类。而且线性回归是线性的，受同类数据中，有
特别远的数据的时候影响很大。使用了logistic(sigmoid）函数。输出的是估计概率。h(x) = P( y=1 | x;Θ）x输入的时候，参数为θ，y=1的概率。
（概率模型？）,损失函数是 J=-y*log(h(x))-(1-y)log(1-h（x))<- 是从统计学中来的，极大似然法，为不同模型找到参数，是凸函数。
（12）Decision Boundary（决策边界）：决策边界是假设函数的属性，他由参数决定。单纯的线性回归是没有决策边界的，他只有输出的确定值，输出
的不是概率信息。
（13）Logistic Regression cost function :出现的原因是因为原来的cost函数，比如是MSE代价函数的话，因加了sigmoid函数后假设函数不再是
线性函数，变形了。会导致代价函数不是凸函数了，此时我们对cost函数做了修改成（11）里面的损失函数。这样代价函数就变成了凸函数。此时标签只有
1和0两种情况。 
（14）Logistic Regression的反向传播公式：θ_j = θ_j - l_r * sum((h(x) - y)*x) ,每一项的输出值减去预测值在乘以输入特征值。
（15）优化算法： （i)Gradient descent （ii）Conjugate gradient (iii) BFGS (iv) L-BFGS
（16）多元分类：（one vs rest) 每个分类器（假设函数，Logistic Regression）对分出一个分类，得到其中一种类的分类输出。每一个分类器只为了
得到一类的预测概率出来。 
（17）正则化后的梯度下降公式后面加了一项-(lambda/m)*(θ_j),每次更新用的θ_J会小一点。
（18）Non-linear Classificatin
（19）多元分类的cost function：需要将四个不同的分类器输出的结果和真实分布求距离，然后在加在一起。目的是让每一个分类器输出的分布都足够接近
真实的分布。
 (20)SVM优化：
 Need fo specify: I)Choice of Parameter C II）Choice of Kernel（similarity function）
 No Kernel（Linear Kernel）的情况下，预测“y=1" 如果θ_T*x>=0 （好像是不使用sigmoid函数了，把预测结果的公式和更新Loss的公式分开了。）
