（1）机器学习算法分为：监督学习，非监督学习，强化学习，推荐系统。
（2）监督学习：包含“正确答案”的数据集，算法的目的就是得出更多的正确答案。
（3）非监督学习：聚类问题和鸡尾酒会问题（音频的分离)
（4）Regression：Predict continuous valued output，线性回归的Cost Function总是趋于Convex Fucntion（凸函数）
（5）Cost Function（代价函数):是关于假设空间的一个函数，是关于假设空间的一个评价准则。最小化代价函数除了梯度下降（迭代的方法）
  正规正方程组法。
（6）矩阵的乘法能够将多个同样的输入特征（行的形式）运用到不同的线性计算（竖的形式）中。
（7）线性回归中一个参数对应一个特征量的计算，不同的样本都是这一个位置的特征量和同一个位置的参数操作。并且受该输入的特征量影响到梯度下降。 
（8）感觉特征缩放类似于Normalization。
（9）多项式回归：假设函数用的是多项式，输入的特征不是经过线性的处理了而是非线性的。 同一个特征经过平方后重新提取出新的数据信息配合新的
参数计算提取特征。
（10）Normal equatin（正规方程）：Method to solve for θ analytically
（11）Logistic Regression：是一个分类算法，线性回归会产生远大于1或者远小于0的值不利于分类。而且线性回归是线性的，受同类数据中，有
特别远的数据的时候影响很大。使用了logistic(sigmoid）函数。输出的是估计概率。h(x) = P( y=1 | x;Θ）x输入的时候，参数为θ，y=1的概率。
（概率模型？）,损失函数是 J=-y*log(h(x))-(1-y)log(1-h（x))<- 是从统计学中来的，极大似然法，为不同模型找到参数，是凸函数。
（12）Decision Boundary（决策边界）：决策边界是假设函数的属性，他由参数决定。单纯的线性回归是没有决策边界的，他只有输出的确定值，输出
的不是概率信息。
